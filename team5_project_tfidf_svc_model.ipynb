{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re, string\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk import FreqDist\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = DataFrame (positive_tweets,columns=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =DataFrame (negative_tweets,columns=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p=pd.read_csv('df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n=pd.read_csv('df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df_p,df_n])\n",
    "df.to_csv('df.csv')\n",
    "#we add the sentiment column after download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 0 to 10001\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0      10000 non-null int64\n",
      "Unnamed: 0.1    10000 non-null object\n",
      "tweets          10000 non-null object\n",
      "sentiment       10000 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 390.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(headline_text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', headline_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets']=df['tweets'].apply(remove_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(headline_text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',headline_text)\n",
    "\n",
    "df['tweets']=df['tweets'].apply(remove_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(headline_text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return headline_text.translate(table)\n",
    "df['tweets']=df['tweets'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return df['tweets']==df1['tweets'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "normalization = None\n",
    "normalization = 'stemmer'\n",
    "normalization = 'lemmatizer'\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "def normalize_tokens(normalization):\n",
    "    if normalization is not None:\n",
    "        if normalization == 'stemmer':\n",
    "            df['tweets']==df['tweets'].apply(stem_tokens)\n",
    "        elif normalization == 'lemmatizer':\n",
    "            df['tweets']==df['tweets'].apply(lemmatize_tokens)\n",
    "        \n",
    "normalize_tokens(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FollowFriday FranceInte PKuchly57 MilipolParis...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Lamb2ja Hey James How odd  Please call our Con...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DespiteOfficial we had a listen last night  As...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>97sides CONGRATS</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>yeaaaah yippppy  my accnt verified rqst has su...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets sentiment\n",
       "0  FollowFriday FranceInte PKuchly57 MilipolParis...  Positive\n",
       "1  Lamb2ja Hey James How odd  Please call our Con...  Positive\n",
       "2  DespiteOfficial we had a listen last night  As...  Positive\n",
       "3                                  97sides CONGRATS   Positive\n",
       "4  yeaaaah yippppy  my accnt verified rqst has su...  Positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"tweets\", \"sentiment\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatureVector(dataset):\n",
    "    featureVector = []\n",
    "\n",
    "    words = dataset.split() # split tweet into words\n",
    "    for w in words:\n",
    "        \n",
    "        w = replaceTwoOrMore(w)  # replace two or more with two occurrences\n",
    "        \n",
    "        w = w.strip('\\'\"?,.') # strip punctuation\n",
    "        \n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w) # check if stats with an alphabet\n",
    "        \n",
    "        if(w in stopWords or val is None): # ignore stop word\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureExtraction():\n",
    "    featureVector = getFeatureVector(dataset)\n",
    "    dataset.append((featureVector, sentiment))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_tweets(dataset):\n",
    "    all_words = []\n",
    "    for (text, sentiment) in dataset:\n",
    "        all_words.extend(text)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist) # frequency distrubtion of all words\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "\n",
    "#word_features = get_word_features(get_words_in_tweets(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataset):\n",
    "    settweet = set(dataset)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in settweet)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"tweets\"]\n",
    "y = df[\"sentiment\"]\n",
    "#count_vect = CountVectorizer()\n",
    "#X=vectorizer.fit_transform(X).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state=42)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# create an instance:\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='squared_hinge', max_iter=1000,\n",
       "                           multi_class='ovr', penalty='l2', random_state=None,\n",
       "                           tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "from sklearn.pipeline import Pipeline\n",
    "                    # vectorize                    # classifier\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC())])\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[756 232]\n",
      " [234 778]]\n"
     ]
    }
   ],
   "source": [
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.77      0.76       988\n",
      "    Positive       0.77      0.77      0.77      1012\n",
      "\n",
      "    accuracy                           0.77      2000\n",
      "   macro avg       0.77      0.77      0.77      2000\n",
      "weighted avg       0.77      0.77      0.77      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))\n",
    "# does vectorizing the tweets get a better performing classifier?\n",
    "# -- COMPARE THESE RESULTS TO THE NAIVE BAYES precision, recall etc ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.767"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweets</th>\n",
       "      <th>date</th>\n",
       "      <th>len</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @aaple_lab: #iPhone 13 (12s) Pro \\n- Design...</td>\n",
       "      <td>2020-12-14 03:22:16</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @TrueWildernesss: #Apple wand, #Swirl apple...</td>\n",
       "      <td>2020-12-14 03:22:01</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>#Apple wand, #Swirl apple wand, Energized wood...</td>\n",
       "      <td>2020-12-14 03:21:47</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>RT @aaple_lab: #iPhone 13 (12s) Pro \\n- Design...</td>\n",
       "      <td>2020-12-14 03:20:46</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>RT @LWNovels: Every chapter is a thriller.  TH...</td>\n",
       "      <td>2020-12-14 03:20:25</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             tweets  \\\n",
       "0           0  RT @aaple_lab: #iPhone 13 (12s) Pro \\n- Design...   \n",
       "1           1  RT @TrueWildernesss: #Apple wand, #Swirl apple...   \n",
       "2           2  #Apple wand, #Swirl apple wand, Energized wood...   \n",
       "3           3  RT @aaple_lab: #iPhone 13 (12s) Pro \\n- Design...   \n",
       "4           4  RT @LWNovels: Every chapter is a thriller.  TH...   \n",
       "\n",
       "                  date  len  likes  retweets sentiment  subjectivity  \n",
       "0  2020-12-14 03:22:16  139      0         9   Neutral          0.12  \n",
       "1  2020-12-14 03:22:01  139      0         1  Negative          0.50  \n",
       "2  2020-12-14 03:21:47  135      0         1  Negative          0.50  \n",
       "3  2020-12-14 03:20:46  139      0         9   Neutral          0.12  \n",
       "4  2020-12-14 03:20:25  140      0         1  Positive          0.00  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip_install tweepy\n",
    "# pip_install pandas\n",
    "# pip_install numpy\n",
    "# pip install matplotlib\n",
    "# pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Our Twitter API Credentials:\n",
    "# (Normally, we store these in a seperate file for precautionary measures.\n",
    "#  However, for the sake of simplicity, we are providing them here.)\n",
    "Consumer_Key = 'j1PZszWJPTn50m9CyZaKQEL4V'\n",
    "Consumer_Secret = '0VLZXNHBidBXOYELBSmAJnWpsohTlMQ2flop7cGY4IRd7gRPc8'        \n",
    "Access_Token = '1334510916344360963-6ESgNupKmKn8yuCEEpi91XLO7cXttC'    \n",
    "Access_Token_Secret = 'O4ghcRniTGXb9V7LIcfW5CpK93Pc6yfTiRY5vxSZReHuq'\n",
    "\n",
    "class Twitter_Customer():\n",
    "\n",
    "    def __init__(self, twitter_user = None):\n",
    "        self.auth = Authentication().authenticate_twitter_app()\n",
    "        self.twitter_client = tweepy.API(self.auth)\n",
    "        self.twitter_user = twitter_user\n",
    "\n",
    "    def twitter_customer_api(self):\n",
    "        return self.twitter_client\n",
    "\n",
    "    \n",
    "class Authentication():\n",
    "\n",
    "    def authenticate_twitter_app(self):\n",
    "        auth = tweepy.OAuthHandler(Consumer_Key, Consumer_Secret)\n",
    "        auth.set_access_token(Access_Token, Access_Token_Secret)\n",
    "        return auth\n",
    "\n",
    "    \n",
    "class My_Stream_Listener():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.twitter_autenticator = Authentication()    \n",
    "\n",
    "    def stream_tweets(self, fetched_tweets_filename):\n",
    "        listener = TwitterListener(fetched_tweets_filename)\n",
    "        auth = self.twitter_autenticator.authenticate_twitter_app() \n",
    "        stream = tweepy.Stream(auth, listener)\n",
    "        \n",
    "class TwitterListener(tweepy.StreamListener):\n",
    "\n",
    "    def __init__(self, fetched_tweets_filename):\n",
    "        self.fetched_tweets_filename = fetched_tweets_filename\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            print(data)\n",
    "            with open(self.fetched_tweets_filename, 'a') as tf:\n",
    "                tf.write(data)\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data %s\" % str(e))\n",
    "        return True\n",
    "          \n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            return False\n",
    "        print(status)\n",
    "        \n",
    "class TweetAnalyzer():\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|(RT[\\s]+)\", \" \", tweet).split())\n",
    "\n",
    "    def analyze_sentiment_polarity(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        \n",
    "        if analysis.sentiment.polarity < 0:\n",
    "            return 'Negative'\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 'Neutral'\n",
    "        else:\n",
    "            return 'Positive'\n",
    "    \n",
    "    def analyze_sentiment_subjectivity(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        return round(analysis.sentiment.subjectivity, 2)\n",
    "\n",
    "    def tweets_to_data_frame(self, tweets):\n",
    "        df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['tweets'])\n",
    "        df['date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "        df['len'] = np.array([len(tweet.text) for tweet in tweets])\n",
    "        df['likes'] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "        df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n",
    "        return df\n",
    " \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    twitter_client = Twitter_Customer()\n",
    "    tweet_analyzer = TweetAnalyzer()\n",
    "\n",
    "    api = twitter_client.twitter_customer_api()\n",
    "\n",
    "    #tweets = api.user_timeline(screen_name = \"TheRock\", count=200)\n",
    "    tweets = api.search(q = \"#Apple\",count = 10000, lang = \"en\")\n",
    "    \n",
    "    df_tweet_api = tweet_analyzer.tweets_to_data_frame(tweets)\n",
    "    df_tweet_api['sentiment'] = np.array([tweet_analyzer.analyze_sentiment_polarity(tweet) for tweet in df_tweet_api['tweets']])\n",
    "    df_tweet_api['subjectivity'] = np.array([tweet_analyzer.analyze_sentiment_subjectivity(tweet) for tweet in df_tweet_api['tweets']])\n",
    "    df_tweet_api.to_csv('twitter_data.csv')\n",
    "\n",
    "df_tweet_api = pd.read_csv('twitter_data.csv')\n",
    "df_tweet_api.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_api=df_tweet_api[df_tweet_api['sentiment']!='Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweet_api.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(headline_text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', headline_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_api['tweets']=df_tweet_api['tweets'].apply(remove_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_api['tweets']=df_tweet_api['tweets'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet_api['tweets']=df_tweet_api['tweets'].apply(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return df_tweet_api['tweets']==df1['tweets'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "normalization = None\n",
    "normalization = 'stemmer'\n",
    "normalization = 'lemmatizer'\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "def normalize_tokens(normalization):\n",
    "    if normalization is not None:\n",
    "        if normalization == 'stemmer':\n",
    "            df_tweet_api['tweets']==df_tweet_api['tweets'].apply(stem_tokens)\n",
    "        elif normalization == 'lemmatizer':\n",
    "            df_tweet_api['tweets']==df_tweet_api['tweets'].apply(lemmatize_tokens)\n",
    "        \n",
    "normalize_tokens(normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_test=df_tweet_api['tweets']\n",
    "y2_test=df_tweet_api['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  4]\n",
      " [12 31]]\n"
     ]
    }
   ],
   "source": [
    "prediciton = text_clf.predict(x2_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(prediciton,y2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.40      0.50        20\n",
      "    Positive       0.72      0.89      0.79        35\n",
      "\n",
      "    accuracy                           0.71        55\n",
      "   macro avg       0.69      0.64      0.65        55\n",
      "weighted avg       0.70      0.71      0.69        55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y2_test, prediciton))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7090909090909091"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y2_test, prediciton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
